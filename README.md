# Кластеризация товаров по частоте изменений

Данный код позволяет выполнить кластеризацию товаров на основе частоты изменений доступного количества. Он использует алгоритм DBSCAN для определения кластеров и визуализирует результаты.

## Зависимости

Для запуска этого кода вам необходимо установить следующие библиотеки:

- pandas
- numpy
- scikit-learn
- matplotlib

```python
pip install pandas numpy scikit-learn matplotlib
```

Если у вас установлена Anaconda, используйте команду conda:

```python
conda install pandas numpy scikit-learn matplotlib
```

## Запуск
1. Скачайте файлы проекта.
2. Установите необходимые зависимости.
4. Добавье в папку проекта файл csv
5. Запустите main.py, используя Python.
7. Введи в терминал навание фалй полностью
8. Ожидайте выволнение

## **python main.py**

**Результаты кластеризации будут сохранены в файл "clustered_data.csv", где указаны URL каждого товара, его кластер и категория изменений.**

## Визуализация
После выполнения кода откроется график, на котором точки представляют товары, раскрашенные в соответствии с их кластерами. Кластеры будут добавлены в виде меток к соответствующим точкам на графике.
По осями X и Y отложенна частота. 
Так же будет второй график гистограмма, кол-ва уникальных предметов в каждом кластере

## Подробно как рабоает алгоритм класторизации

1. Конвертация даты и времени в нужный формат:
   - Код `data['created'] = pd.to_datetime(data['created'])` конвертирует столбец 'created' в формат datetime, чтобы обеспечить правильное представление даты и времени.

2. Группировка данных по URL и подсчет разницы в доступном количестве и временной разницы:
   - Код `data["available_quantity_diff"] = data.groupby("url")["available_quantity"].diff().abs()` вычисляет разницу в доступном количестве (столбец 'available_quantity') между последовательными записями для каждого URL.
   - Код `data["time_diff"] = data.groupby("url")["created"].diff().abs().dt.total_seconds()` вычисляет разницу во времени (столбец 'created') между последовательными записями для каждого URL, представленную в секундах.

3. Вычисление частоты изменения:
   - Код `data["frequency"] = data["available_quantity_diff"] / data["time_diff"]` вычисляет частоту изменений путем деления разницы в доступном количестве на разницу во времени.

4. Удаление возможных пропущенных значений:
   - Код `data.dropna(subset=["frequency"], inplace=True)` удаляет строки с пропущенными значениями в столбце 'frequency'.

5. Выделение данных для кластеризации:
   - Код `X = data[["frequency"]].values` создает матрицу признаков X, содержащую только один столбец 'frequency'.

6. Нормализация данных:
   - Код `X = StandardScaler().fit_transform(X)` нормализует данные с помощью метода StandardScaler, чтобы привести их к нулевому среднему и единичной дисперсии.

7. Кластеризация данных с помощью метода DBSCAN:
   - Код `dbscan = DBSCAN(eps=0.5, min_samples=5)` создает экземпляр DBSCAN с заданными параметрами.
   - Код `data["cluster"] = dbscan.fit_predict(X)` применяет метод DBSCAN к данным и назначает метки кластеров каждой записи в новый столбец "cluster".

8. Назначение скоростей изменений по категориям:
   - Код `data["change_rate"] = pd.cut(data["frequency"], bins=[-np.inf] + thresholds + [np.inf], labels=labels)` разбивает значения столбца "frequency" на указанные индексами интервалы в соответствии со списками порогов `thresholds` и метками `labels`. Этим создается категориальная переменная "change_rate", отражающая скорость изменений.

9. Сохранение результатов в файл "clustered_data.csv":
   - Код `data[["url", "cluster", "change_rate"]].to_csv("clustered_data.csv", index=False)` сохраняет столбцы "url", "cluster" и "change_rate" в файл "clustered_data.csv".

# Для справки 

Параметры eps и min_samples являются основными параметрами для метода DBSCAN (Density-Based Spatial Clustering of Applications with Noise), которые определяют его поведение. Вот их объяснение:

- eps (epsilon): Это параметр, который указывает радиус окрестности вокруг каждой точки. DBSCAN формирует кластеры путем соединения точек, которые находятся внутри этого радиуса. Точки, находящиеся в пределах радиуса eps, считаются соседними точками. Значение eps определяет, насколько плотно расположены точки в кластере. Большое значение eps означает, что точки в далеких областях могут быть объединены в один кластер, в то время как маленькое значение eps может привести к разделению кластеров на несколько более плотных областей. Вы должны подобрать значение eps в зависимости от свойств вашего набора данных.

- min_samples: Это параметр, который указывает минимальное количество точек, находящихся внутри радиуса eps, чтобы считать это ядром кластера. Если в пределах радиуса eps находится меньшее количество точек, чем min_samples, эти точки считаются выбросами и не принадлежат ни одному кластеру. Значение min_samples влияет на размер и плотность кластеров. Большее значение min_samples означает, что кластеры будут иметь большую плотность и меньше выбросов, в то время как меньшее значение min_samples может давать более широкие и разреженные кластеры. Также, как и с eps, параметр min_samples нужно настраивать в соответствии с характеристиками своего набора данных.
